{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -Use several technical indicators to build features from price\n",
    "#### -Use Random Forest model to extract a subset of features.\n",
    "#### -Use Extreme Gradient Boosting Tree as a prediction model. \n",
    "#### -Rolling window is used in prediction and model is retrained every time\n",
    "#### -The validation dataset has a trick in this strategy. It is used for the optimization of the prediction power with the feature selection of Random Forest  at every time point ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the library and data\n",
    "import pymysql   \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyti.exponential_moving_average import exponential_moving_average as ema\n",
    "from pyti.relative_strength_index import relative_strength_index as rsi\n",
    "from pyti import stochastic as stoc\n",
    "from pyti.williams_percent_r import williams_percent_r as wr\n",
    "from pyti.rate_of_change import rate_of_change as roc\n",
    "from pyti.commodity_channel_index import commodity_channel_index as cci\n",
    "from pyti.simple_moving_average import simple_moving_average as sma\n",
    "from pyti.momentum import momentum as mom\n",
    "from pyti.volatility import volatility as vol\n",
    "from pyti.average_true_range_percent import average_true_range_percent as atrp\n",
    "from pyti.detrended_price_oscillator import detrended_price_oscillator as dpo\n",
    "from pyti.chande_momentum_oscillator import chande_momentum_oscillator as cmo\n",
    "from pyti.directional_indicators import average_directional_index as adi\n",
    "from pyti.double_smoothed_stochastic import double_smoothed_stochastic as dss\n",
    "from pyti.stochrsi import stochrsi\n",
    "from arch import arch_model\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix as table_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.svm\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "\n",
    "\n",
    "conn = pymysql.connect(host=<IP>,  \n",
    "                       port=3306,  \n",
    "                       user=<USER>,  \n",
    "                       passwd=<PW>,  \n",
    "                       db=<db>,  \n",
    "                       charset='utf8')  \n",
    "\n",
    "\n",
    "sql = \"SELECT * FROM CU where ContractName='CU_Index';\"  \n",
    "CU = pd.read_sql(sql, conn)[['StrDateTime','OPEN','HIGH','LOW','LAST_PRICE']]\n",
    "CU.columns=['times','open','high','low','close']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Functions \n",
    "def read_data(data,field):\n",
    "    data_0=data\n",
    "    data_0['times'] = pd.to_datetime(data_0.times)\n",
    "    data_0 = data_0.sort_values(by='times')\n",
    "    data_0.index=data_0['times'].values       \n",
    "    #data_0.index=data_0['times']\n",
    "    data_0=data_0[field].resample('1H',label='right').ohlc() #data frequency\n",
    "    data_0['times']=data_0.index\n",
    "    data_0.dropna(inplace=True)\n",
    "    return data_0\n",
    "def slope(data, period):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    from pyti import catch_errors\n",
    "    from pyti.function_helper import fill_for_noncomputable_vals\n",
    "    from six.moves import range    \n",
    "    catch_errors.check_for_period_error(data, period)\n",
    "    rocs =[((data[idx] - data[idx - (period - 1)]) / (period - 1))  for idx in range(period - 1, len(data))]\n",
    "    rocs = fill_for_noncomputable_vals(data, rocs)\n",
    "    return rocs\n",
    "\n",
    "def rolling_garch(returns,n,p,o,q):\n",
    "    vol=np.repeat(np.nan,n)\n",
    "    for i in range(n,returns.shape[0]):\n",
    "        x=returns[(i-n):(i)]\n",
    "        am = arch_model(x, vol='Garch', p=p, o=o, q=q, dist='Normal').fit()\n",
    "        vol=np.append(vol,am.conditional_volatility.values[-1])   \n",
    "    return vol\n",
    "        \n",
    "def data_prep(dataset):\n",
    "    ema_diff=ema(dataset['close'],12)-ema(dataset['close'],26)\n",
    "    ema_diff_2=ema(dataset['close'],24)-ema(dataset['close'],75)\n",
    "    returns=np.log(dataset['close']/dataset['close'].shift(1))   \t\n",
    "    dataset=dataset.assign(**{\n",
    "                            'EMA_1':ema_diff,       \n",
    "                            'EMA_2':ema(dataset['close'],30)-ema(dataset['close'],90), \n",
    "                            'EMA_3':ema(dataset['close'],20)-ema(dataset['close'],60), \n",
    "                            'EMA_4':ema(dataset['close'],40)-ema(dataset['close'],130),\n",
    "                            'EMA_5':ema(dataset['close'],18)-ema(dataset['close'],54), \t\t\t\t   \n",
    "                            'MACD_1':ema_diff - ema(ema_diff,9),\n",
    "                            'MACD_2':(ema_diff_2)- ema(ema_diff_2,12),\t\t\t\t\t   \n",
    "                            'MOM_1':mom(dataset['close'],3),\t\t\t\t\t    \n",
    "                            'MOM_2':mom(dataset['close'],10),\t\t\t\t\t   \n",
    "                            'MOM_3':mom(dataset['close'],20),\t\t\t\t       \t\t\t\t\t    \n",
    "                            'MOM_4':mom(dataset['close'],25),\n",
    "                            'VOL_1':vol(dataset['close'],3),\n",
    "                            'VOL_2':vol(dataset['close'],10),\t\t\t\t\t    \n",
    "                            'VOL_3':vol(dataset['close'],23),    \n",
    "                            'VOL_4':vol(dataset['close'],35),\t\t\t\t\t    \n",
    "                            'RSI_1':rsi(dataset['close'],14),\t\t\t\t\t    \n",
    "                            'RSI_2':rsi(dataset['close'],12),\t\t\t\t\t   \n",
    "                            'RSI_3':rsi(dataset['close'],10),\t\t\t\t\t    \n",
    "                            'RSI_4':rsi(dataset['close'],8),\t\t\t\t\t    \n",
    "                            'RSI_5':rsi(dataset['close'],7),\t\t\t\t\t    \n",
    "                            'STOC_1':stoc.percent_k(dataset['close'],20)-stoc.percent_d(dataset['close'],20),\t\t\t\t\t    \n",
    "                            'STOC_2':stoc.percent_k(dataset['close'],40)-stoc.percent_d(dataset['close'],40),\t\t\t\t\t     \t\t\n",
    "                            'STOC_3':stoc.percent_k(dataset['close'],33)-stoc.percent_d(dataset['close'],33),\n",
    "                            'STOC_4':stoc.percent_k(dataset['close'],12)-stoc.percent_d(dataset['close'],12),\t\t\t\t\t    \n",
    "                            'STOC_5':stoc.percent_k(dataset['close'],25)-stoc.percent_d(dataset['close'],25),\t\t\t\t\t    \n",
    "                            'WR':wr(dataset['close']),\t\t\t\t\t    \n",
    "                            'ROC_1':roc(dataset['close'],7),\t\t\t\t\t    \t\n",
    "                            'ROC_2':roc(dataset['close'],12),\n",
    "                            'ROC_3':roc(dataset['close'],5),\n",
    "                            'ROC_4':roc(dataset['close'],13),\n",
    "                            'ROC_5':roc(dataset['close'],10),\n",
    "                            'ATRP_1':atrp(dataset['close'],3),\t                 \t\t\t\t\t\n",
    "                            'ATRP_2':atrp(dataset['close'],7),\n",
    "                            'ATRP_3':atrp(dataset['close'],14),\t                 \n",
    "                            'ATRP_4':atrp(dataset['close'],30),\t                 \n",
    "                            'ATRP_5':atrp(dataset['close'],50),\t                 \n",
    "                            'CCI_1':cci(dataset['close'],dataset['high'],dataset['low'],20),\t                 \n",
    "                            'CCI_2':cci(dataset['close'],dataset['high'],dataset['low'],15),\t                 \n",
    "                            'CCI_3':cci(dataset['close'],dataset['high'],dataset['low'],7),\t                 \n",
    "                            'CCI_4':cci(dataset['close'],dataset['high'],dataset['low'],11),\t                 \n",
    "                            'CCI_5':cci(dataset['close'],dataset['hignh'],dataset['low'],10),\t                 \n",
    "                            'DPO_1':dpo(dataset['close'],5),\t\n",
    "                            'DPO_2':dpo(dataset['close'],10),\n",
    "                            'DPO_3':dpo(dataset['close'],20),\n",
    "                            'DPO_4':dpo(dataset['close'],30),\n",
    "                            'DPO_5':dpo(dataset['close'],40),\n",
    "                            'CMO_1':cmo(dataset['close'],5),\n",
    "                            'CMO_2':cmo(dataset['close'],10),\n",
    "                            'CMO_3':cmo(dataset['close'],20),\n",
    "                            'CMO_4':cmo(dataset['close'],30),\n",
    "                            'CMO_5':cmo(dataset['close'],40),\n",
    "                            'ADI_1':adi(dataset['close'],dataset['high'],dataset['low'],5),\n",
    "                            'ADI_2':adi(dataset['close'],dataset['high'],dataset['low'],10),\n",
    "                            'ADI_3':adi(dataset['close'],dataset['high'],dataset['low'],20),\n",
    "                            'ADI_4':adi(dataset['close'],dataset['high'],dataset['low'],30),\n",
    "                            'ADI_5':adi(dataset['close'],dataset['high'],dataset['low'],40),\n",
    "                            'DSS_1':dss(dataset['close'],5),  \n",
    "                            'DSS_2':dss(dataset['close'],10),\n",
    "                            'DSS_3':dss(dataset['close'],20),\n",
    "                            'DSS_4':dss(dataset['close'],30),\n",
    "                            'DSS_5':dss(dataset['close'],40),\n",
    "                            'STOCHRSI_1':stochrsi(dataset['close'],5),\n",
    "                            'STOCHRSI_2':stochrsi(dataset['close'],14),  \n",
    "                            'STOCHRSI_3':stochrsi(dataset['close'],20),\n",
    "                            'STOCHRSI_4':stochrsi(dataset['close'],30),\n",
    "                            'STOCHRSI_5':stochrsi(dataset['close'],40),\n",
    "                            'H_1':dataset['high'],\n",
    "                            'H_2':sma(dataset['high'],5),\n",
    "                            'H_3':sma(dataset['high'],15),\n",
    "                            'L_1':sma(dataset['low'],1),\n",
    "                            'L_2':sma(dataset['low'],5),\n",
    "                            'L_3':sma(dataset['low'],15),\n",
    "                            'slope_1':slope(dataset['high'],3),   \n",
    "                            'slope_2':slope(dataset['high'],6),\n",
    "                            'slope_3':slope(dataset['high'],9),\n",
    "                            'slope_4':slope(dataset['high'],12),\n",
    "                            'slope_5':slope(dataset['high'],15),\n",
    "                            'slope_6':slope(dataset['high'],18),\n",
    "                            'slope_7':slope(dataset['low'],3),\n",
    "                            'slope_8':slope(dataset['low'],6),\n",
    "                            'slope_9':slope(dataset['low'],9),\n",
    "                            'slope_10':slope(dataset['low'],12),\n",
    "                            'slope_11':slope(dataset['low'],15),\n",
    "                            'slope_12':slope(dataset['low'],18),   \n",
    "                            'returns':np.log(dataset['close']/dataset['close'].shift(1)),\n",
    "                            'GARCH_1':rolling_garch(returns,20*8,1,0,1),\n",
    "                            'GARCH_2':rolling_garch(returns,20*8,3,0,1),\n",
    "                            'GARCH_3':rolling_garch(returns,20*8,2,0,1),\n",
    "                            'label':(np.log(dataset['close']/dataset['close'].shift(1))).shift(-1)})\n",
    "    #'GARCH_4':rolling_garch(returns,20*8,1,0,2)\n",
    "    #'GARCH_5':rolling_garch(returns,20*8,1,0,3)\n",
    "    #'GARCH_6':rolling_garch(returns,20*8,1,1,1)\n",
    "    #'GARCH_7':rolling_garch(returns,20*8,1,2,1)\n",
    "    #'GARCH_8':rolling_garch(returns,20*8,1,3,1)\n",
    "    #'GARCH_9':rolling_garch(returns,20*8,2,1,1)\n",
    "    #'GARCH_10':rolling_garch(returns,20*8,3,1,1)\n",
    "    #'GARCH_11':rolling_garch(returns,20*8,1,1,2)\n",
    "    #'GARCH_12':rolling_garch(returns,20*8,1,1,3)\n",
    "    #'GARCH_13':rolling_garch(returns,20*8,4,0,1)\n",
    "    up=dataset['label'].quantile(0.66)\n",
    "    down=dataset['label'].quantile(0.33)    \n",
    "    def S_row (row):\n",
    "        if row >=up :\n",
    "            return 3\n",
    "        elif row <= down :\n",
    "            return 1\n",
    "        elif row < up and row > down:\n",
    "            return 2\n",
    "    dataset['label'] = dataset['label'].apply(lambda row: S_row(row)) \n",
    "    return dataset                 \n",
    "\n",
    "features=['EMA_1',\n",
    "'EMA_2',\n",
    "'EMA_3',\n",
    "'EMA_4',\n",
    "'EMA_5',\n",
    "'MACD_1',\n",
    "'MACD_2',\n",
    "'MOM_1',\n",
    "'MOM_2',\n",
    "'MOM_3',\n",
    "'MOM_4',\n",
    "'VOL_1',\n",
    "'VOL_2',\n",
    "'VOL_3',\n",
    "'VOL_4',\n",
    "'RSI_2',\n",
    "'RSI_3',\n",
    "'RSI_4',\n",
    "'RSI_5',\n",
    "'STOC_1',\n",
    "'STOC_2',\n",
    "'STOC_3',\n",
    "'STOC_4',\n",
    "'STOC_5',\n",
    "'WR',\n",
    "'ROC_1',\n",
    "'ROC_2',\n",
    "'ROC_3',\n",
    "'ROC_4',\n",
    "'ROC_5',\n",
    "'ATRP_1',\n",
    "'ATRP_2',\n",
    "'ATRP_3',\n",
    "'ATRP_4',\n",
    "'ATRP_5',\n",
    "'CCI_1',\n",
    "'CCI_2',\n",
    "'CCI_3',\n",
    "'CCI_4',\n",
    "'CCI_5',\n",
    "'DPO_1',\n",
    "'DPO_2',\n",
    "'DPO_3',\n",
    "'DPO_4',\n",
    "'DPO_5',\n",
    "'CMO_1',\n",
    "'CMO_2',\n",
    "'CMO_3',\n",
    "'CMO_4',\n",
    "'CMO_5',\n",
    "'ADI_1',\n",
    "'ADI_2',\n",
    "'ADI_3',\n",
    "'ADI_4',\n",
    "'ADI_5',\n",
    "'DSS_1',\n",
    "'DSS_2',\n",
    "'DSS_3',\n",
    "'DSS_4',\n",
    "'DSS_5',\n",
    "'STOCHRSI_1',\n",
    "'STOCHRSI_2',\n",
    "'STOCHRSI_3',\n",
    "'STOCHRSI_4',\n",
    "'STOCHRSI_5',\n",
    "'H_1',\n",
    "'H_2',\n",
    "'H_3',\n",
    "'L_1',\n",
    "'L_2',\n",
    "'L_3',\n",
    "'slope_1',\n",
    "'slope_2',\n",
    "'slope_3',\n",
    "'slope_4',\n",
    "'slope_5',\n",
    "'slope_6',\n",
    "'slope_7',\n",
    "'slope_8',\n",
    "'slope_9',\n",
    "'slope_10',\n",
    "'slope_11',\n",
    "'slope_12',\n",
    "'returns',\n",
    "'GARCH_1',\n",
    "'GARCH_2',\n",
    "'GARCH_3'\n",
    "#'GARCH_4',\n",
    "#'GARCH_5',\n",
    "#'GARCH_6',\n",
    "#'GARCH_7',\n",
    "#'GARCH_8',\n",
    "#'GARCH_9',\n",
    "#'GARCH_10',\n",
    "#'GARCH_11',\n",
    "#'GARCH_12',\n",
    "#'FFT_1',\n",
    "#\"EMD_1\",\n",
    "#\"EMD_2\",\n",
    "#\"EMD_3\",\n",
    "#\"EMD_4\",\n",
    "#\"EMD_5\",\n",
    "#\"EMD_6\",\n",
    "#\"EMD_7\",\n",
    "#\"EMD_8\",\n",
    "#\"EMD_9\",\n",
    "#\"EMD_10\",\n",
    "#\"EMD_11\",\n",
    "#\"EMD_12\",\n",
    "#\"EMD_13\",\n",
    "#\"EMD_14\",\n",
    "#\"EMD_15\",\n",
    "#\"EMD_16\",\n",
    "#\"EMD_17\"\n",
    "]\n",
    "\n",
    "def feature_selection(dataset,features,n):\n",
    "    model = RFC()\n",
    "    #model = ExtraTreesClassifier()\n",
    "    X=dataset[features]\n",
    "    Y=dataset['label']\n",
    "    model.fit(X, Y)\n",
    "    feats = {} # a dict to hold feature_name: feature_importance\n",
    "    for feature, importance in zip(features, model.feature_importances_):\n",
    "        feats[feature] = importance #add the name/value pair \n",
    "    importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "    choose_features=importances.sort_values(by='Gini-importance').tail(n)\n",
    "    print(importances.sort_values(by='Gini-importance'))\n",
    "    return choose_features.reset_index()['index'].values\n",
    "    \n",
    "def XGB_train(dataset,choose_features):\n",
    "    train=dataset\n",
    "    #val=dataset.iloc[round(dataset.shape[0]*0.9):dataset.shape[0],] \n",
    "    X=train[choose_features].values\n",
    "    y=train[['label']].values\n",
    "    #X_val=val[choose_features].values\n",
    "    #y_val=val[['label']].values\n",
    "    dtrain=xgb.DMatrix(data=X,label=y)\n",
    "    #dval=xgb.DMatrix(data=X_val,label=y_val)\n",
    "    #for non-scale data, 30 mins    \n",
    "    param = {'max_depth': 3, 'gamma': 0.0001, 'eta':0.8, 'silent': 1, 'objective': 'multi:softmax',\n",
    "             'num_class': 4,'n_estimators': 100,'seed':20} \n",
    "    param['nthread'] = 1\n",
    "    param['eval_metric'] = 'mlogloss'\n",
    "    evallist = [(dtrain, 'train')]\n",
    "    num_round =100\n",
    "    bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "    return bst\n",
    "\n",
    "def XGB_predict(test,bst_model,choose_features):\n",
    "    X=xgb.DMatrix(test[choose_features].values)\n",
    "    pred=bst_model.predict(X, ntree_limit=bst_model.best_ntree_limit)\n",
    "    return pred\n",
    "\n",
    "    \n",
    "def opt_XGB(train,test,features_list):\n",
    "    score={'features':[0],'accuracy':[0]}\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    a,b,c=0.0,0.0,0\n",
    "    while True:       \n",
    "        if max(score['accuracy'])<=1.1 and c<3:\n",
    "            for i in range(10):\n",
    "                choose_features=feature_selection(dataset=train,features=features_list,n=20) #choose features  \n",
    "                bst_model=XGB_train(train,choose_features)\n",
    "                pred=XGB_predict(test,bst_model,choose_features) \n",
    "                matrix=table_matrix(test['label'],pred)\n",
    "                a=(matrix[0,0]+matrix[2,2])/(matrix[0,0]+matrix[2,2]+matrix[0,2]+matrix[2,0])     \n",
    "                b=accuracy_score(test['label'],pred)\n",
    "                score['features'].append(choose_features)\n",
    "                score['accuracy'].append(a+b)\n",
    "            c+=1\n",
    "        else:\n",
    "            print('next data point')\n",
    "            break\n",
    "    position_=max(score['accuracy'])\n",
    "    index_=[i for i,x in enumerate(score['accuracy']) if x == position_]    \n",
    "    features=score['features'][index_[0]]\n",
    "    print(position_)\n",
    "    return features\n",
    "\n",
    "def rolling_XGB_test(dataset,train,validate,test,features):\n",
    "    pred=np.repeat(np.nan,test.shape[0])\n",
    "    train_len=train.shape[0]\n",
    "    validate_len=validate.shape[0]\n",
    "    test_len=test.shape[0]\n",
    "    dataset.reset_index(drop=True,inplace=True)\n",
    "    for i in range(0,test.shape[0]):\n",
    "        temp_validate=dataset.loc[(train_len+i):(train_len+validate_len+i),]\n",
    "        temp_validate.reset_index(drop=True,inplace=True)\n",
    "        temp_train=dataset.loc[0:(train_len+i)]\n",
    "        temp_train.reset_index(drop=True,inplace=True)\n",
    "        temp_test=dataset.loc[(train_len+validate_len+i):(train_len+validate_len+test_len)]\n",
    "        temp_test.reset_index(drop=True,inplace=True)\n",
    "        choose_features=opt_XGB(temp_train,temp_validate,features)\n",
    "        bst_model=XGB_train(temp_train,choose_features)#train\n",
    "        pred[i]=XGB_predict(temp_test,bst_model,choose_features)[0]\n",
    "    return pred\n",
    "        \n",
    "def backtest(dataset,pred,i):\n",
    "    table=pd.DataFrame(np.zeros(shape=(dataset.shape[0],3)))\n",
    "    table.columns=[[\"StartPrice_0\",\"EndPrice_0\",\"position_0\"]]\n",
    "    table['times']=dataset['times'].values\n",
    "    table['open']=dataset['open'].values\n",
    "    table['CumCap_0']=10000\n",
    "    table.dropna(inplace=True)\n",
    "    table.reset_index(inplace=True)\n",
    "    label=0\n",
    "    a=[0]     \n",
    "    for j in range(1,table.shape[0]-1):\n",
    "        label=pred[j]-2                      \n",
    "        if (label == 1) & (table.loc[j-1,'position_0'][0]==0):\n",
    "            print('open + position')\n",
    "            table.loc[(j+1):table.shape[0],\"StartPrice_0\"]=table.loc[j+1,'open'].values[0]\n",
    "            table.loc[(j):table.shape[0],\"position_0\"]=1\n",
    "        elif (label == -1) & (table.loc[j-1,'position_0'][0]==0):\n",
    "            print('open - position')\n",
    "            table.loc[(j+1):table.shape[0],\"StartPrice_0\"]=table.loc[j+1,'open'].values[0]\n",
    "            table.loc[(j):table.shape[0],\"position_0\"]=-1               \n",
    "        elif (label == 0) & (table.loc[j-1,'position_0'][0] !=0) | (i==table.shape[0]):\n",
    "            print('close all position')\n",
    "            table.loc[(j):table.shape[0],\"EndPrice_0\"]=table.loc[j+1,'open'].values[0]\n",
    "            table.loc[(j):table.shape[0],\"CumCap_0\"]=table.loc[j-1,'CumCap_0'].values[0]*(table.loc[j,'EndPrice_0'].values[0]/table.loc[j,'StartPrice_0'].values[0])**(table.loc[j-1,\"position_0\"].values[0])-table.loc[j-1,'CumCap_0'].values[0]*0.0001  \n",
    "            #table.loc[(j):table.shape[0],\"CumCap_0\"]=table.loc[j-1,'CumCap_0']+10000*20*(table.loc[j,'EndPrice_0']/table.loc[j,'StartPrice_0'])**(table.loc[j-1,\"position_0\"])-10000*20-10000*20*(0.01/table.loc[j,'StartPrice_0']+0.01/table.loc[j,'EndPrice_0'])\n",
    "            a.append(1)\n",
    "            table.loc[(j):table.shape[0],\"position_0\"]=0\n",
    "        elif (label != table.loc[j-1,'position_0'][0]) & (table.loc[j-1,'position_0'][0] != 0) & (label != 0):\n",
    "            print('reverse position')\n",
    "            table.loc[(j):table.shape[0],\"EndPrice_0\"]=table.loc[j+1,'open'].values[0]\n",
    "            table.loc[(j):table.shape[0],\"CumCap_0\"]=table.loc[j-1,'CumCap_0'].values[0]*(table.loc[j,'EndPrice_0'].values[0]/table.loc[j,'StartPrice_0'].values[0])**(table.loc[j-1,\"position_0\"].values[0])-table.loc[j-1,'CumCap_0'].values[0]*0.0001   \n",
    "            #table.loc[(j):table.shape[0],\"CumCap_0\"]=table.loc[j-1,'CumCap_0']+10000*20*(table.loc[j,'EndPrice_0']/table.loc[j,'StartPrice_0'])**(table.loc[j-1,\"position_0\"])-10000*20-10000*20*(0.01/table.loc[j,'StartPrice_0']+0.01/table.loc[j,'EndPrice_0'])    \n",
    "            a.append(1)\n",
    "            table.loc[(j+1):table.shape[0],\"StartPrice_0\"]=table.loc[j+1,'open'].values[0]\n",
    "            table.loc[(j):table.shape[0],\"position_0\"]= label\n",
    "        else:\n",
    "            print('do nothing')\n",
    "        print(table.shape[0]-j)    \n",
    "    return table,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame()\n",
    "dataset=dataset.assign(**{'times':read_data(CU,'open')['times'].values,'open':read_data(CU,'open')['open'].values,\n",
    "                          'high':read_data(CU,'high')['high'].values,'low':read_data(CU,'low')['low'].values,\n",
    "                          'close':read_data(CU,'close')['close'].values})\n",
    "dataset=dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.index=dataset['times']\n",
    "dataset=data_prep(dataset)\n",
    "dataset=dataset.dropna()\n",
    "\n",
    "train=dataset.iloc[0:(dataset.shape[0]-120),]\n",
    "validate=dataset.iloc[(dataset.shape[0]-120):(dataset.shape[0]-80),] #one week  #40 datapoints\n",
    "test=dataset.iloc[(dataset.shape[0]-80):(dataset.shape[0]),]\n",
    "\n",
    "pred=rolling_XGB_test(dataset,train,validate,test,features)    \n",
    "matrix=table_matrix(test['label'],pred)\n",
    "print((matrix[0,0] + matrix[2,2])/(matrix[0,0] + matrix[2,2] + matrix[0,2] + matrix[2,0]))\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test['label'],pred)\n",
    "\n",
    "result,a=backtest(test,pred,0)\n",
    "result['CumCap_0'].plot(title='80 hrs cumulative PNL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
